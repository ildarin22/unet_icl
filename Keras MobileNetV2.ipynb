{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune MobileNetV2 on a new set of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.mobilenetv2 import MobileNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Activation at 0x1d3591bbfd0>,\n",
       " <keras.layers.merge.Concatenate at 0x1d359551940>,\n",
       " <keras.layers.merge.Concatenate at 0x1d3599c89b0>,\n",
       " <keras.layers.core.Activation at 0x1d359aa0f98>,\n",
       " <keras.layers.merge.Concatenate at 0x1d359b02c88>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_inceptionv3_model = InceptionV3(weights=None, include_top=False)\n",
    "new_inceptionv3_model.layers[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.merge.Concatenate at 0x2562c5e17f0>,\n",
       " <keras.layers.core.Activation at 0x2562c6afbe0>,\n",
       " <keras.layers.merge.Concatenate at 0x2562c723f28>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x2562c6e2e80>,\n",
       " <keras.layers.core.Dense at 0x2562c6e2e10>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_inceptionv3_model = InceptionV3(weights=None, include_top=True)\n",
    "base_inceptionv3_model.layers[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to load model without top layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x1dc41f0c208>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1dc41fc3d68>,\n",
       " <keras.layers.advanced_activations.ReLU at 0x1dc4200ba90>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x1dc42071f60>,\n",
       " <keras.layers.core.Dense at 0x1dc4209cef0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model= MobileNetV2(input_shape=(224, 224, 3), alpha=1.4, depth_multiplier=1.0, include_top=True, weights='imagenet')\n",
    "base_model.layers[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x1d35dfc8b38>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1d35e0153c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1d35e0bdf98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1d35e1306a0>,\n",
       " <keras.layers.advanced_activations.ReLU at 0x1d35e180400>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the base pre-trained model\n",
    "new_model = MobileNetV2(input_shape=(224, 224, 3), alpha=1.4, depth_multiplier=1.0, include_top=False, weights='imagenet', \n",
    "                        pooling=None)\n",
    "new_model.layers[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = GlobalAveragePooling2D()(new_model.output)\n",
    "# let's add a fully-connected layer\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 10 classes\n",
    "predictions = Dense(10, activation='softmax', name='softmax')(x)\n",
    "# output = Reshape((num_classes,))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x1d35e0bdf98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1d35e1306a0>,\n",
       " <keras.layers.advanced_activations.ReLU at 0x1d35e180400>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x1d360743cc0>,\n",
       " <keras.layers.core.Dense at 0x1d35e2343c8>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = Model(inputs=new_model.input, outputs=predictions)\n",
    "new_model.layers[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(base_model.input, base_model.output, new_model.input, new_model.output, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 images belonging to 10 classes.\n",
      "Found 10000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# add callbacks\n",
    "# highly optional in case of xiauchus GitHub\n",
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor='val_acc', patience=30, verbose=0, mode='auto')\n",
    "\n",
    "def generate(batch, size):\n",
    "    \"\"\"Data generation and augmentation\n",
    "\n",
    "    # Arguments\n",
    "        batch: Integer, batch size.\n",
    "        size: Integer, image size.\n",
    "\n",
    "    # Returns\n",
    "        train_generator: train set generator\n",
    "        validation_generator: validation set generator\n",
    "        count1: Integer, number of train set.\n",
    "        count2: Integer, number of test set.\n",
    "    \"\"\"\n",
    "\n",
    "    #  Using the data Augmentation in traning data\n",
    "    ptrain = 'data/train'\n",
    "    pval = 'data/validation'\n",
    "\n",
    "    datagen1 = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=90,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    datagen2 = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    train_generator = datagen1.flow_from_directory(\n",
    "        ptrain,\n",
    "        target_size=(size, size),\n",
    "        batch_size=batch,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    validation_generator = datagen2.flow_from_directory(\n",
    "        pval,\n",
    "        target_size=(size, size),\n",
    "        batch_size=batch,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    count1 = 0\n",
    "    for root, dirs, files in os.walk(ptrain):\n",
    "        for each in files:\n",
    "            count1 += 1\n",
    "\n",
    "    count2 = 0\n",
    "    for root, dirs, files in os.walk(pval):\n",
    "        for each in files:\n",
    "            count2 += 1\n",
    "\n",
    "    return train_generator, validation_generator, count1, count2\n",
    "\n",
    "#batch = 128\n",
    "batch = 256\n",
    "size = 224\n",
    "train_generator, validation_generator, count1, count2 = generate(batch, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we chose to train the top 3 layers, i.e. we will freeze the first 155 layers and unfreeze the rest:\n",
    "for layer in new_model.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "for layer in new_model.layers[-2:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_4\n",
      "1 Conv1_pad\n",
      "2 Conv1\n",
      "3 bn_Conv1\n",
      "4 Conv1_relu\n",
      "5 expanded_conv_depthwise\n",
      "6 expanded_conv_depthwise_BN\n",
      "7 expanded_conv_depthwise_relu\n",
      "8 expanded_conv_project\n",
      "9 expanded_conv_project_BN\n",
      "10 block_1_expand\n",
      "11 block_1_expand_BN\n",
      "12 block_1_expand_relu\n",
      "13 block_1_pad\n",
      "14 block_1_depthwise\n",
      "15 block_1_depthwise_BN\n",
      "16 block_1_depthwise_relu\n",
      "17 block_1_project\n",
      "18 block_1_project_BN\n",
      "19 block_2_expand\n",
      "20 block_2_expand_BN\n",
      "21 block_2_expand_relu\n",
      "22 block_2_depthwise\n",
      "23 block_2_depthwise_BN\n",
      "24 block_2_depthwise_relu\n",
      "25 block_2_project\n",
      "26 block_2_project_BN\n",
      "27 block_2_add\n",
      "28 block_3_expand\n",
      "29 block_3_expand_BN\n",
      "30 block_3_expand_relu\n",
      "31 block_3_pad\n",
      "32 block_3_depthwise\n",
      "33 block_3_depthwise_BN\n",
      "34 block_3_depthwise_relu\n",
      "35 block_3_project\n",
      "36 block_3_project_BN\n",
      "37 block_4_expand\n",
      "38 block_4_expand_BN\n",
      "39 block_4_expand_relu\n",
      "40 block_4_depthwise\n",
      "41 block_4_depthwise_BN\n",
      "42 block_4_depthwise_relu\n",
      "43 block_4_project\n",
      "44 block_4_project_BN\n",
      "45 block_4_add\n",
      "46 block_5_expand\n",
      "47 block_5_expand_BN\n",
      "48 block_5_expand_relu\n",
      "49 block_5_depthwise\n",
      "50 block_5_depthwise_BN\n",
      "51 block_5_depthwise_relu\n",
      "52 block_5_project\n",
      "53 block_5_project_BN\n",
      "54 block_5_add\n",
      "55 block_6_expand\n",
      "56 block_6_expand_BN\n",
      "57 block_6_expand_relu\n",
      "58 block_6_pad\n",
      "59 block_6_depthwise\n",
      "60 block_6_depthwise_BN\n",
      "61 block_6_depthwise_relu\n",
      "62 block_6_project\n",
      "63 block_6_project_BN\n",
      "64 block_7_expand\n",
      "65 block_7_expand_BN\n",
      "66 block_7_expand_relu\n",
      "67 block_7_depthwise\n",
      "68 block_7_depthwise_BN\n",
      "69 block_7_depthwise_relu\n",
      "70 block_7_project\n",
      "71 block_7_project_BN\n",
      "72 block_7_add\n",
      "73 block_8_expand\n",
      "74 block_8_expand_BN\n",
      "75 block_8_expand_relu\n",
      "76 block_8_depthwise\n",
      "77 block_8_depthwise_BN\n",
      "78 block_8_depthwise_relu\n",
      "79 block_8_project\n",
      "80 block_8_project_BN\n",
      "81 block_8_add\n",
      "82 block_9_expand\n",
      "83 block_9_expand_BN\n",
      "84 block_9_expand_relu\n",
      "85 block_9_depthwise\n",
      "86 block_9_depthwise_BN\n",
      "87 block_9_depthwise_relu\n",
      "88 block_9_project\n",
      "89 block_9_project_BN\n",
      "90 block_9_add\n",
      "91 block_10_expand\n",
      "92 block_10_expand_BN\n",
      "93 block_10_expand_relu\n",
      "94 block_10_depthwise\n",
      "95 block_10_depthwise_BN\n",
      "96 block_10_depthwise_relu\n",
      "97 block_10_project\n",
      "98 block_10_project_BN\n",
      "99 block_11_expand\n",
      "100 block_11_expand_BN\n",
      "101 block_11_expand_relu\n",
      "102 block_11_depthwise\n",
      "103 block_11_depthwise_BN\n",
      "104 block_11_depthwise_relu\n",
      "105 block_11_project\n",
      "106 block_11_project_BN\n",
      "107 block_11_add\n",
      "108 block_12_expand\n",
      "109 block_12_expand_BN\n",
      "110 block_12_expand_relu\n",
      "111 block_12_depthwise\n",
      "112 block_12_depthwise_BN\n",
      "113 block_12_depthwise_relu\n",
      "114 block_12_project\n",
      "115 block_12_project_BN\n",
      "116 block_12_add\n",
      "117 block_13_expand\n",
      "118 block_13_expand_BN\n",
      "119 block_13_expand_relu\n",
      "120 block_13_pad\n",
      "121 block_13_depthwise\n",
      "122 block_13_depthwise_BN\n",
      "123 block_13_depthwise_relu\n",
      "124 block_13_project\n",
      "125 block_13_project_BN\n",
      "126 block_14_expand\n",
      "127 block_14_expand_BN\n",
      "128 block_14_expand_relu\n",
      "129 block_14_depthwise\n",
      "130 block_14_depthwise_BN\n",
      "131 block_14_depthwise_relu\n",
      "132 block_14_project\n",
      "133 block_14_project_BN\n",
      "134 block_14_add\n",
      "135 block_15_expand\n",
      "136 block_15_expand_BN\n",
      "137 block_15_expand_relu\n",
      "138 block_15_depthwise\n",
      "139 block_15_depthwise_BN\n",
      "140 block_15_depthwise_relu\n",
      "141 block_15_project\n",
      "142 block_15_project_BN\n",
      "143 block_15_add\n",
      "144 block_16_expand\n",
      "145 block_16_expand_BN\n",
      "146 block_16_expand_relu\n",
      "147 block_16_depthwise\n",
      "148 block_16_depthwise_BN\n",
      "149 block_16_depthwise_relu\n",
      "150 block_16_project\n",
      "151 block_16_project_BN\n",
      "152 Conv_1\n",
      "153 Conv_1_bn\n",
      "154 out_relu\n",
      "155 global_average_pooling2d_2\n",
      "156 dense_1\n",
      "157 softmax\n"
     ]
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(new_model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "195/195 [==============================] - 12589s 65s/step - loss: 1.5001 - acc: 0.4788 - val_loss: 1.6317 - val_acc: 0.4510\n",
      "Epoch 2/5\n",
      "195/195 [==============================] - 12333s 63s/step - loss: 1.2328 - acc: 0.5731 - val_loss: 1.5955 - val_acc: 0.4740\n",
      "Epoch 3/5\n",
      "195/195 [==============================] - 12329s 63s/step - loss: 1.1809 - acc: 0.5876 - val_loss: 1.6992 - val_acc: 0.4640\n",
      "Epoch 4/5\n",
      "195/195 [==============================] - 12375s 63s/step - loss: 1.1557 - acc: 0.5961 - val_loss: 1.6422 - val_acc: 0.4710\n",
      "Epoch 5/5\n",
      "195/195 [==============================] - 12374s 63s/step - loss: 1.1359 - acc: 0.6054 - val_loss: 1.6086 - val_acc: 0.4827\n"
     ]
    }
   ],
   "source": [
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "hist = new_model.fit_generator(train_generator, \n",
    "                               steps_per_epoch=count1 // batch, \n",
    "                               epochs=5, \n",
    "                               verbose=1, \n",
    "                               validation_data=validation_generator, \n",
    "                               validation_steps=count2 // batch, \n",
    "                               callbacks=[earlystop], \n",
    "                               class_weight=None, \n",
    "                               max_queue_size=10, \n",
    "                               workers=1, \n",
    "                               use_multiprocessing=False, \n",
    "                               shuffle=True, \n",
    "                               initial_epoch=0)\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "\n",
    "df = pd.DataFrame.from_dict(hist.history)\n",
    "df.to_csv('model/new_model_hist.csv', encoding='utf-8', index=False)\n",
    "new_model.save_weights('model/new_model_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mobiolenetv2)",
   "language": "python",
   "name": "mobilenetv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
